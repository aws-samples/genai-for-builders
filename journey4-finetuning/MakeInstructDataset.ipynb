{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d3ee57-7ee8-47a9-bd3a-09aaa438a775",
   "metadata": {},
   "source": [
    "# Create an Instruction Tuned Dataset from Plain Text\n",
    "\n",
    "by `@sanjivda`\n",
    "\n",
    "One of the pain points of instruction fine tuning is to prepare a dataset in the form of `Instruction + Context + Ground Truth`. This is usually done manually. Indeed, DataBricks prepared the [Dolly](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm) dataset by crowd-sourcing it among its employees.\n",
    "\n",
    "This is a lot of work that can easily be replicated by careful prompting of an LLM. We show that this is effective on a large sample of text transcripts from quarterly earnings calls of companies. We used the Claude2 LLM from Bedrock. You will need access to Bedrock for this but no other knowledge, other than setting up the permissions. The code in this notebook takes care of all else re Bedrock, but these are useful docs for reference in any case.\n",
    "\n",
    "References:\n",
    "\n",
    "- https://github.com/aws-samples/amazon-bedrock-workshop\n",
    "- https://docs.aws.amazon.com/bedrock/latest/userguide/api-methods-run-inference.html#api-inference-examples-meta-llama\n",
    "\n",
    "The approach is a simple two-step hack.\n",
    "\n",
    "1. Prompt the LLM to generate question (`instruction`) based on some text (`context`).\n",
    "2. The prompt also asks the LLM to then provide the answer (`response`) to the question.\n",
    "\n",
    "Here is an example using Amazon Bedrock with Claude-2. It works remarkably well!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6b97f56-e7e3-4de2-8785-0ee935913aee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f44a1b-a66c-4002-8744-b02f98ce6499",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 5962, which is longer than the specified 500\n",
      "Created a chunk of size 1596, which is longer than the specified 500\n",
      "Created a chunk of size 662, which is longer than the specified 500\n",
      "Created a chunk of size 557, which is longer than the specified 500\n",
      "Created a chunk of size 547, which is longer than the specified 500\n",
      "Created a chunk of size 550, which is longer than the specified 500\n",
      "Created a chunk of size 553, which is longer than the specified 500\n",
      "Created a chunk of size 553, which is longer than the specified 500\n",
      "Created a chunk of size 553, which is longer than the specified 500\n",
      "Created a chunk of size 538, which is longer than the specified 500\n",
      "Created a chunk of size 564, which is longer than the specified 500\n",
      "Created a chunk of size 798, which is longer than the specified 500\n",
      "Created a chunk of size 745, which is longer than the specified 500\n",
      "Created a chunk of size 998, which is longer than the specified 500\n",
      "Created a chunk of size 602, which is longer than the specified 500\n",
      "Created a chunk of size 676, which is longer than the specified 500\n",
      "Created a chunk of size 618, which is longer than the specified 500\n",
      "Created a chunk of size 537, which is longer than the specified 500\n",
      "Created a chunk of size 517, which is longer than the specified 500\n",
      "Created a chunk of size 523, which is longer than the specified 500\n",
      "Created a chunk of size 633, which is longer than the specified 500\n",
      "Created a chunk of size 507, which is longer than the specified 500\n",
      "Created a chunk of size 622, which is longer than the specified 500\n",
      "Created a chunk of size 810, which is longer than the specified 500\n",
      "Created a chunk of size 507, which is longer than the specified 500\n",
      "Created a chunk of size 507, which is longer than the specified 500\n",
      "Created a chunk of size 716, which is longer than the specified 500\n",
      "Created a chunk of size 502, which is longer than the specified 500\n",
      "Created a chunk of size 507, which is longer than the specified 500\n",
      "Created a chunk of size 553, which is longer than the specified 500\n",
      "Created a chunk of size 749, which is longer than the specified 500\n",
      "Created a chunk of size 684, which is longer than the specified 500\n",
      "Created a chunk of size 509, which is longer than the specified 500\n",
      "Created a chunk of size 749, which is longer than the specified 500\n",
      "Created a chunk of size 513, which is longer than the specified 500\n",
      "Created a chunk of size 599, which is longer than the specified 500\n",
      "Created a chunk of size 572, which is longer than the specified 500\n",
      "Created a chunk of size 512, which is longer than the specified 500\n",
      "Created a chunk of size 573, which is longer than the specified 500\n",
      "Created a chunk of size 616, which is longer than the specified 500\n",
      "Created a chunk of size 589, which is longer than the specified 500\n",
      "Created a chunk of size 622, which is longer than the specified 500\n",
      "Created a chunk of size 589, which is longer than the specified 500\n",
      "Created a chunk of size 739, which is longer than the specified 500\n",
      "Created a chunk of size 515, which is longer than the specified 500\n",
      "Created a chunk of size 551, which is longer than the specified 500\n",
      "Created a chunk of size 602, which is longer than the specified 500\n",
      "Created a chunk of size 971, which is longer than the specified 500\n",
      "Created a chunk of size 565, which is longer than the specified 500\n",
      "Created a chunk of size 586, which is longer than the specified 500\n",
      "Created a chunk of size 633, which is longer than the specified 500\n",
      "Created a chunk of size 659, which is longer than the specified 500\n",
      "Created a chunk of size 502, which is longer than the specified 500\n",
      "Created a chunk of size 546, which is longer than the specified 500\n",
      "Created a chunk of size 641, which is longer than the specified 500\n",
      "Created a chunk of size 622, which is longer than the specified 500\n",
      "Created a chunk of size 615, which is longer than the specified 500\n",
      "Created a chunk of size 608, which is longer than the specified 500\n",
      "Created a chunk of size 589, which is longer than the specified 500\n",
      "Created a chunk of size 641, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"../dataset/unsupervised-fine-tuning/train.txt\") as f:\n",
    "    file = f.read()\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=50\n",
    ")\n",
    "texts = text_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ed3dcee3-0f50-4187-a7b5-de03fd181434",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2823"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15a99f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '68ade209-1005-485f-becc-ae627adfc3ed', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Jan 2024 14:53:03 GMT', 'content-type': 'application/json', 'content-length': '17086', 'connection': 'keep-alive', 'x-amzn-requestid': '68ade209-1005-485f-becc-ae627adfc3ed'}, 'RetryAttempts': 0}, 'modelSummaries': [{'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-tg1-large', 'modelId': 'amazon.titan-tg1-large', 'modelName': 'Titan Text Large', 'providerName': 'Amazon', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-g1-text-02', 'modelId': 'amazon.titan-embed-g1-text-02', 'modelName': 'Titan Text Embeddings v2', 'providerName': 'Amazon', 'inputModalities': ['TEXT'], 'outputModalities': ['EMBEDDING'], 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-text-lite-v1:0:4k', 'modelId': 'amazon.titan-text-lite-v1:0:4k', 'modelName': 'Titan Text G1 - Lite', 'providerName': 'Amazon', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': ['FINE_TUNING', 'CONTINUED_PRE_TRAINING'], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-text-lite-v1', 'modelId': 'amazon.titan-text-lite-v1', 'modelName': 'Titan Text G1 - Lite', 'providerName': 'Amazon', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-text-express-v1:0:8k', 'modelId': 'amazon.titan-text-express-v1:0:8k', 'modelName': 'Titan Text G1 - Express', 'providerName': 'Amazon', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': ['FINE_TUNING', 'CONTINUED_PRE_TRAINING'], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-text-express-v1', 'modelId': 'amazon.titan-text-express-v1', 'modelName': 'Titan Text G1 - Express', 'providerName': 'Amazon', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-text-v1:2:8k', 'modelId': 'amazon.titan-embed-text-v1:2:8k', 'modelName': 'Titan Embeddings G1 - Text', 'providerName': 'Amazon', 'inputModalities': ['TEXT'], 'outputModalities': ['EMBEDDING'], 'responseStreamingSupported': False, 'customizationsSupported': [], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-text-v1', 'modelId': 'amazon.titan-embed-text-v1', 'modelName': 'Titan Embeddings G1 - Text', 'providerName': 'Amazon', 'inputModalities': ['TEXT'], 'outputModalities': ['EMBEDDING'], 'responseStreamingSupported': False, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-image-v1:0', 'modelId': 'amazon.titan-embed-image-v1:0', 'modelName': 'Titan Multimodal Embeddings G1', 'providerName': 'Amazon', 'inputModalities': ['TEXT', 'IMAGE'], 'outputModalities': ['EMBEDDING'], 'customizationsSupported': ['FINE_TUNING'], 'inferenceTypesSupported': ['ON_DEMAND', 'PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-image-v1', 'modelId': 'amazon.titan-embed-image-v1', 'modelName': 'Titan Multimodal Embeddings G1', 'providerName': 'Amazon', 'inputModalities': ['TEXT', 'IMAGE'], 'outputModalities': ['EMBEDDING'], 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-image-generator-v1:0', 'modelId': 'amazon.titan-image-generator-v1:0', 'modelName': 'Titan Image Generator G1', 'providerName': 'Amazon', 'inputModalities': ['TEXT', 'IMAGE'], 'outputModalities': ['IMAGE'], 'customizationsSupported': ['FINE_TUNING'], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-image-generator-v1', 'modelId': 'amazon.titan-image-generator-v1', 'modelName': 'Titan Image Generator G1', 'providerName': 'Amazon', 'inputModalities': ['TEXT', 'IMAGE'], 'outputModalities': ['IMAGE'], 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/stability.stable-diffusion-xl', 'modelId': 'stability.stable-diffusion-xl', 'modelName': 'SDXL 0.8', 'providerName': 'Stability AI', 'inputModalities': ['TEXT', 'IMAGE'], 'outputModalities': ['IMAGE'], 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/stability.stable-diffusion-xl-v0', 'modelId': 'stability.stable-diffusion-xl-v0', 'modelName': 'SDXL 0.8', 'providerName': 'Stability AI', 'inputModalities': ['TEXT', 'IMAGE'], 'outputModalities': ['IMAGE'], 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/stability.stable-diffusion-xl-v1:0', 'modelId': 'stability.stable-diffusion-xl-v1:0', 'modelName': 'SDXL 1.0', 'providerName': 'Stability AI', 'inputModalities': ['TEXT', 'IMAGE'], 'outputModalities': ['IMAGE'], 'customizationsSupported': [], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/stability.stable-diffusion-xl-v1', 'modelId': 'stability.stable-diffusion-xl-v1', 'modelName': 'SDXL 1.0', 'providerName': 'Stability AI', 'inputModalities': ['TEXT', 'IMAGE'], 'outputModalities': ['IMAGE'], 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-grande-instruct', 'modelId': 'ai21.j2-grande-instruct', 'modelName': 'J2 Grande Instruct', 'providerName': 'AI21 Labs', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': False, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-jumbo-instruct', 'modelId': 'ai21.j2-jumbo-instruct', 'modelName': 'J2 Jumbo Instruct', 'providerName': 'AI21 Labs', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': False, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-mid', 'modelId': 'ai21.j2-mid', 'modelName': 'Jurassic-2 Mid', 'providerName': 'AI21 Labs', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': False, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-mid-v1', 'modelId': 'ai21.j2-mid-v1', 'modelName': 'Jurassic-2 Mid', 'providerName': 'AI21 Labs', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': False, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-ultra', 'modelId': 'ai21.j2-ultra', 'modelName': 'Jurassic-2 Ultra', 'providerName': 'AI21 Labs', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': False, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-ultra-v1', 'modelId': 'ai21.j2-ultra-v1', 'modelName': 'Jurassic-2 Ultra', 'providerName': 'AI21 Labs', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': False, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-instant-v1:2:100k', 'modelId': 'anthropic.claude-instant-v1:2:100k', 'modelName': 'Claude Instant', 'providerName': 'Anthropic', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-instant-v1', 'modelId': 'anthropic.claude-instant-v1', 'modelName': 'Claude Instant', 'providerName': 'Anthropic', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v1', 'modelId': 'anthropic.claude-v1', 'modelName': 'Claude', 'providerName': 'Anthropic', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'LEGACY'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2:0:18k', 'modelId': 'anthropic.claude-v2:0:18k', 'modelName': 'Claude', 'providerName': 'Anthropic', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2:0:100k', 'modelId': 'anthropic.claude-v2:0:100k', 'modelName': 'Claude', 'providerName': 'Anthropic', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2:1:18k', 'modelId': 'anthropic.claude-v2:1:18k', 'modelName': 'Claude', 'providerName': 'Anthropic', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2:1:200k', 'modelId': 'anthropic.claude-v2:1:200k', 'modelName': 'Claude', 'providerName': 'Anthropic', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2:1', 'modelId': 'anthropic.claude-v2:1', 'modelName': 'Claude', 'providerName': 'Anthropic', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2', 'modelId': 'anthropic.claude-v2', 'modelName': 'Claude', 'providerName': 'Anthropic', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.command-text-v14:7:4k', 'modelId': 'cohere.command-text-v14:7:4k', 'modelName': 'Command', 'providerName': 'Cohere', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': ['FINE_TUNING'], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.command-text-v14', 'modelId': 'cohere.command-text-v14', 'modelName': 'Command', 'providerName': 'Cohere', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.command-light-text-v14:7:4k', 'modelId': 'cohere.command-light-text-v14:7:4k', 'modelName': 'Command Light', 'providerName': 'Cohere', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': ['FINE_TUNING'], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.command-light-text-v14', 'modelId': 'cohere.command-light-text-v14', 'modelName': 'Command Light', 'providerName': 'Cohere', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.embed-english-v3', 'modelId': 'cohere.embed-english-v3', 'modelName': 'Embed English', 'providerName': 'Cohere', 'inputModalities': ['TEXT'], 'outputModalities': ['EMBEDDING'], 'responseStreamingSupported': False, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.embed-multilingual-v3', 'modelId': 'cohere.embed-multilingual-v3', 'modelName': 'Embed Multilingual', 'providerName': 'Cohere', 'inputModalities': ['TEXT'], 'outputModalities': ['EMBEDDING'], 'responseStreamingSupported': False, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-13b-chat-v1:0:4k', 'modelId': 'meta.llama2-13b-chat-v1:0:4k', 'modelName': 'Llama 2 Chat 13B', 'providerName': 'Meta', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-13b-chat-v1', 'modelId': 'meta.llama2-13b-chat-v1', 'modelName': 'Llama 2 Chat 13B', 'providerName': 'Meta', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-70b-chat-v1:0:4k', 'modelId': 'meta.llama2-70b-chat-v1:0:4k', 'modelName': 'Llama 2 Chat 70B', 'providerName': 'Meta', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': [], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-70b-chat-v1', 'modelId': 'meta.llama2-70b-chat-v1', 'modelName': 'Llama 2 Chat 70B', 'providerName': 'Meta', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-13b-v1:0:4k', 'modelId': 'meta.llama2-13b-v1:0:4k', 'modelName': 'Llama 2 13B', 'providerName': 'Meta', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': ['FINE_TUNING'], 'inferenceTypesSupported': [], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-13b-v1', 'modelId': 'meta.llama2-13b-v1', 'modelName': 'Llama 2 13B', 'providerName': 'Meta', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': [], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-70b-v1:0:4k', 'modelId': 'meta.llama2-70b-v1:0:4k', 'modelName': 'Llama 2 70B', 'providerName': 'Meta', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': ['FINE_TUNING'], 'inferenceTypesSupported': [], 'modelLifecycle': {'status': 'ACTIVE'}}, {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-70b-v1', 'modelId': 'meta.llama2-70b-v1', 'modelName': 'Llama 2 70B', 'providerName': 'Meta', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': [], 'modelLifecycle': {'status': 'ACTIVE'}}]}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "from pprint import pprint\n",
    "\n",
    "bedrock_client = boto3.client(service_name=\"bedrock\", region_name=\"us-west-2\")\n",
    "print(bedrock_client.list_foundation_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d0a7ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "bdrk_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "\n",
    "def call_bedrock(bedrock_runtime, modelId, prompt_data):\n",
    "    if \"amazon\" in modelId:\n",
    "        body = json.dumps(\n",
    "            {\n",
    "                \"inputText\": prompt_data,\n",
    "                \"textGenerationConfig\": {\n",
    "                    \"maxTokenCount\": 4096,\n",
    "                    \"stopSequences\": [],\n",
    "                    \"temperature\": 0,\n",
    "                    \"topP\": 0.9,\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "        # modelId = 'amazon.titan-tg1-large'\n",
    "    elif \"anthropic\" in modelId:\n",
    "        body = json.dumps(\n",
    "            {\n",
    "                \"prompt\": prompt_data,\n",
    "                \"max_tokens_to_sample\": 20000,\n",
    "                \"temperature\": 0.5,\n",
    "                \"top_p\": 0.9,\n",
    "                \"stop_sequences\": [\"\\n\\nHuman:\"],\n",
    "            }\n",
    "        )\n",
    "        # modelId = 'anthropic.claude-instant-v1'\n",
    "    elif \"ai21\" in modelId:\n",
    "        body = json.dumps({\"prompt\": prompt_data, \"maxTokens\": 4096})\n",
    "        # modelId = 'ai21.j2-grande-instruct'\n",
    "    elif \"stability\" in modelId:\n",
    "        body = json.dumps({\"text_prompts\": [{\"text\": prompt_data}]})\n",
    "        # modelId = 'stability.stable-diffusion-xl'\n",
    "    else:\n",
    "        print(\"Parameter model must be one of titan, claude, j2, or sd\")\n",
    "        return\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    before = datetime.now()\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    latency = datetime.now() - before\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    if \"amazon\" in modelId:\n",
    "        response = response_body.get(\"results\")[0].get(\"outputText\")\n",
    "    elif \"anthropic\" in modelId:\n",
    "        response = response_body.get(\"completion\")\n",
    "    elif \"ai21\" in modelId:\n",
    "        response = response_body.get(\"completions\")[0].get(\"data\").get(\"text\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2b6e2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first president of the United States was George Washington. He took office in 1789 after leading the Continental Army in the American Revolutionary War and presiding over the Constitutional Convention. Washington served two four-year terms as president from 1789-1797.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_bedrock(\n",
    "    bdrk_runtime,\n",
    "    \"anthropic.claude-v2:1\",\n",
    "    \"\\n\\nHuman:Who was the first president of the United States?\\n\\nAssistant:\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c95406d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"instruction\": \"What efforts is Apple making to increase market share in countries like India where market penetration is lower compared to developed countries?\",\n",
    "        \"context\": \"Good day, and welcome to the Apple Q1 Fiscal Year 2021 Earnings Conference Call. Today's call is being recorded. At this time, for opening remarks and introductions, I would like to turn the call over to Tejas Gala, Director of Investor Relations and Corporate Finance. Please go ahead.  We'll go question from Katy Huberty with Morgan Stanley. Please go ahead.  We'll hear next from Wamsi Mohan with Bank of America. Please go ahead.  We'll go ahead and take our next question from Shannon Cross with Cross Research.  We'll hear from Toni Sacconaghi with Bernstein.  We'll hear from Amit Daryanani with Evercore ISI. Please go ahead.  We'll go ahead and take our next question from Samik Chatterjee with JP Morgan.  We'll hear from Krish Sankar with Cowen.  We'll go ahead and take our next question from Chris Caso with Raymond James. Please go ahead.  We'll go ahead and hear from Jim Suva with Citigroup. James Suva It's amazing how your company has pivoted and progressed through this uncertain time in society. A lot of the pushback we get on our view on Apple is that everyone around them or that they know developed countries has an iPhone or Apple product and the market is kind of being saturated some. But when I look at other countries like India, I believe statistically, you are materially below that in market share. So are you doing active efforts there? It seems like there's been some news reports of moving supply chain there. Or you recently opened up an Apple Store. How should we think about that? Because it just seems like you're really not full market share equally around the world.  Once again, that does conclude today's conference. We do appreciate your participation. \",\n",
    "        \"response\": \"Apple is actively working to increase market share in India through measures like moving supply chain operations there, opening Apple Stores, and recent news reports. India represents a major growth opportunity for Apple since smartphone penetration is much lower compared to developed countries where many consumers already own iPhones.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Can you talk a bit about what's going on with services growth, including factors impacting acceleration and deceleration?\",\n",
    "        \"context\": \"Thank you very much. Can you talk a bit about what's going on within services, some of the puts and takes? Luca, you gave us some color in terms of the growth rates in that, but I'm just curious and I know you won't talk about future products, but as you think about the opportunity, you think about what you've got now and in the future and then some of what’s been going on with China and that, is this something that could reaccelerate, or again, the 18% on a constant currency basis is obviously quite strong. But, how are you thinking about it? Luca Maestri Yes. I think it's important to start with that 18% in constant currencies, Shannon. Our reported results are on a normalized basis, removing the one-time item from last year, was 15%. Clearly FX, plays a role around the world, 300 basis points of FX impact during the June quarter. In spite of that, it was an all-time record revenue. Our installed base continues to grow. It's growing in every geography and it's growing across all our major product categories, and that is very, very important for the services business. I would say, I'll give you a bit more color around two offsetting factors around this performance during the June quarter. On one side, the App Store, I mentioned in my prepared remarks that growth accelerated sequentially. We had double-digit growth on the App Store in every geography. In China, we saw significant acceleration. As you know, we tend to monetize in China on the App Store through game titles and the government has approved a few key game titles during the quarter. That has helped our performance there. On the other side, AppleCare, I mentioned AppleCare was an all-time record in June, so really strong performance, but our growth has decelerated in AppleCare due to factors that we fully expected, because we are comping this expansion of our coverage for AppleCare that we've had significant success during the last 18 to 24 months in really broadening our coverage of AppleCare around the world with some key partners, carriers and resellers. And obviously, as we go through the year those comps become a bit more difficult. Having said all that, that we've given ourselves a couple of targets and we feel very confident about reaching those targets. The first one is that, we wanted to double the size of the services business from our fiscal 2016 to 2020. We are on our way there. Paid subscriptions is another target, is important to us. It's an important way for us to monetize our ecosystem. We set a target of surpassing 0.5 billion paid subscriptions on the ecosystem during 2020. We’re already at 420 million now. So, we feel confident there. And of course, as you mentioned, we're very excited about the fact that we're going to be launching new services soon. As Tim said, we're starting the rollout of Apple Card in August, and there's two more very important services that we’re going to be adding to our portfolio during the fall. One is Apple Arcade, which is our gaming subscription service, and then of course Apple TV+, which is our video streaming service. So obviously these services will help us carry on with the momentum that we had in services.  Great. Thank you. And this is probably for you too as well, Luca. Can you talk about gross margin? The guidance was pretty solid. Obviously, there are various things that are at play here. I know you mentioned $1 billion worth of top-line impact, I think from, currency last quarter. Then maybe if you can kind of talk about what went into your gross margin guidance. Luca Maestri Yes. So of course, Shannon, as you've seen, our guidance for margin is 50 basis points higher than the guidance that we had given for June. I would say on the positive, we expect to benefit from leverage. As you've seen from our revenue guidance, and from cost savings, because as you know the commodity environment is fairly favorable right now. On the negative side, the headwind on gross margins on a year-over-year basis from foreign exchange is about 100 basis points. And so, we need to keep that in mind. But we feel pretty good about the guidance we provided. Nancy Paxton Thanks Shannon. Could we have the next question please. \",\n",
    "        \"response\": \"Luca Maestri provided color on services growth. The App Store accelerated globally and saw significant acceleration in China as new game titles were approved. AppleCare set an all-time revenue record but growth decelerated as Apple lapped expansion of coverage over the past 18-24 months. New services like Apple Card, Apple Arcade, and Apple TV+ will help continue momentum. Services remains on track to double in size from 2016 to 2020 and surpass 500 million paid subscriptions in 2020.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"How has Apple been impacted by supply chain disruptions over the past couple of years, and has this led the company to rethink its manufacturing footprint and broader supply chain strategy going forward?\",\n",
    "        \"context\": \"So first question just as it relates to some of the disruption you've seen on the component side, manufacturing and logistics over the past couple of years, are you starting to rethink your broader supply chain strategy or the manufacturing footprint on the back of the significant disruption? Are you happy with the overall geographic exposure that you see in the supply chain today?  Okay. And how are you thinking about the metaverse opportunity and Apple's role in that market? \",\n",
    "        \"response\": \"Apple has faced component shortages and logistics challenges during the pandemic, but it's unclear if this will lead to major changes in Apple's supply chain strategy or manufacturing footprint. The company seems satisfied with its geographic exposure but may make tweaks to enhance resilience.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Tim, four years ago you predicted services would reach $50 billion in revenue by 2020. Do you have an updated prediction for services revenue four years from now?\",\n",
    "        \"context\": \"Hi, thanks for taking my question. I have two of them. First one, Tim, when you look at the services business and in terms of your TV+ content production have the movement restrictions impacted the content production efforts? And along the same path four years ago your premonition on services being a $50 billion business in 2020 came sooner than expected, I don't know if you want to make any such forecast four years out and how you think services revenue is going to be. Then I have a follow-up for Luca.  Yes, Tim, I was trying to see, four years ago you made a great prediction that services is going to be $50 billion by 2020. I wanted to see if you have any update to the prediction four years down the road?  If I could just squeeze in one for Luca. With the strong sales in Mac given the shelter-in-place, do you think the back-to-school season got pulled in by a quarter or do you expect the momentum to still continue? Thank you very much. \",\n",
    "        \"response\": \"Four years ago, Tim Cook predicted Apple's services revenue would reach $50 billion by 2020, which it did. He has not made any similar prediction looking four years ahead, but services revenue continues to grow rapidly for Apple.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Why is Apple's revenue guidance for the upcoming quarter lower than historical trends despite enthusiasm for new iPhone, Apple Watch, and services products? Is Apple being conservative given economic uncertainty, or are other factors at play?\",\n",
    "        \"context\": \"Yes. Thank you. I think this is for Luca, and then I have a follow-up as well. If I look at your guidance, the midpoint of your guidance for revenues on a sequential basis, it's up about 36.5%. Historically, fiscal Q4 to Q1 was up 50% or more, and even last year given that iPhone is a slower growing product, you guided for revenues to be up 45% sequentially. So given the enthusiasm about the iPhone 11 launch and the new Wearables products and the new services, I guess the question is, why is your guidance not stronger for Q1 on the top line? And is that sort of a reflection of conservatism given that there's a lot of uncertainty in the world and we certainly saw that last year? Or are there other forces at work that we should be considered?  Okay. Thank you for that. And then if I could follow-up, just on the bundling of Apple TV+, I guess for you, Tim, this is really the first time we've seen a significant bundling of services offering and hardware offering. And I'm wondering if you view this as kind of a strategic advantage of Apple and whether we might see more hardware plus services offerings - bundled offerings, and ultimately do you - do you ever believe that your hardware itself might be offered as a bundled service. And maybe while we're on that either you or Luca could give us the 30 second tutorial on how we should think about the deferred revenue accounting? Approximately how much of the $60 are you going to be deferring, and what's your expectation for attach rate on that? Thank you. \",\n",
    "        \"response\": \"Apple's revenue guidance for the upcoming quarter is lower than past years' trends despite new product launches. This could reflect conservatism by Apple given economic uncertainty globally. It may also indicate other factors influencing demand, like longer upgrade cycles for iPhones. Apple may be taking a cautious approach to guidance given unpredictability in the current environment.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "sample_question = examples[2][\"instruction\"]\n",
    "sample_context = examples[2][\"context\"]\n",
    "sample_answer = examples[2][\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce7bebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple version of the prompt\n",
    "\n",
    "import re\n",
    "\n",
    "PromptTemplate = \"\"\"\\n\\nHuman:\\\n",
    "Below is an snippet of a financial document.\\n\n",
    "\n",
    "Snippet:\\n\n",
    "<context>\n",
    "{context}</context>\\n\\n\n",
    "\n",
    "Given the context information above generate only questions based on the below query.\n",
    "\n",
    "You are a Professor teaching finance at University. You had good success with teaching your students how to extract data from earnings transcripts.\n",
    "There is an examples below. Your task is to setup new questions for an upcoming \\\n",
    "quiz/examination based on financial documents like 10k or 8k statements. The questions should be diverse in nature and difficulty \\\n",
    "across the database. You can create more difficult questions than shown above.  The questions should not contain options, not start with Q1/ Q2. \\\n",
    "Try to create different levels of complexity for the questions. Some where they purely need to find some data, others where they need to infer the context \\\n",
    "Output only the question and the corresponding answers between <NewPair></NewPair> tags!\\\n",
    "Try to create 1 to 3 questions per snippet, try to vary the difficulty!\n",
    "\n",
    "The questions can simply be who was hired, who leads a division etc. Students should just get familiar with extracting information from the documents. \\\n",
    "\n",
    "Create a new questions and answers based on the above query. Do not repeat the same questions.\n",
    "Remember, only output the question and the corresponding answers between <NewPair></NewPair> tags! I need to parse them afterwards, so any other text is unwanted!\n",
    "\n",
    "An example of a question and answer pair and its format is shown below based on the earnings call transcripts:\\n\n",
    "\n",
    "    <sample_context>{sample_context}</sample_context>\n",
    "\n",
    "    <NewPair>\n",
    "    <question>{sample_question}</question>\n",
    "    <answer>{sample_answer}</answer>\n",
    "    </NewPair>\n",
    "\n",
    "\\n\\nAssistant:\\\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def parse_output(output):\n",
    "    # Define a regex pattern to capture each pair of question, answer, and complexity\n",
    "    pattern = r\"<NewPair>\\s*<question>(.*?)</question>\\s*<answer>\\s*(.*?)\\s*</answer>\\s*</NewPair>\"\n",
    "    # Find all matches in the output\n",
    "    matches = re.findall(pattern, output, re.DOTALL)\n",
    "    return matches\n",
    "\n",
    "\n",
    "for text in texts:\n",
    "    # Fill the prompt template\n",
    "    prompt = PromptTemplate.format(\n",
    "        context=texts[0],\n",
    "        sample_context=sample_context,\n",
    "        sample_question=sample_question,\n",
    "        sample_answer=sample_answer,\n",
    "    )\n",
    "\n",
    "    # Generate the questions\n",
    "    response = call_bedrock(bdrk_runtime, \"anthropic.claude-v2:1\", prompt)\n",
    "\n",
    "    # Parse the questions\n",
    "    questions = parse_output(response)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b06fe960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"What is the start date for Joseph Galli Jr's employment with Amazon.com?\",\n",
       "  'June 24, 1999'),\n",
       " (\"How much was Joseph Galli Jr's initial salary at Amazon.com?\",\n",
       "  '$16,666.70 per month ($200,000 annualized)'),\n",
       " ('What was the amount of the signing bonus paid to Joseph Galli Jr in 3 installments?',\n",
       "  '$7,900,000'),\n",
       " ('What was the number of shares in the initial stock option grant to Joseph Galli Jr that vests over 10 years?',\n",
       "  '735,000 shares'),\n",
       " ('What was the number of shares in the additional stock option grant to Joseph Galli Jr that vests after 10 years?',\n",
       "  '1,225,000 shares')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5555fe",
   "metadata": {},
   "source": [
    "### Non parallelized version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63fcd37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# def parse_output(output):\n",
    "#     pattern = r\"<NewPair>\\s*<question>(.*?)</question>\\s*<answer>\\s*(.*?)\\s*</answer>\\s*</NewPair>\"\n",
    "#     matches = re.findall(pattern, output, re.DOTALL)\n",
    "#     return matches\n",
    "\n",
    "\n",
    "# # DataFrame to store the results\n",
    "# df = pd.DataFrame(columns=[\"Context\", \"Question\", \"Answer\"])\n",
    "\n",
    "# for text in texts:\n",
    "#     prompt = PromptTemplate.format(\n",
    "#         context=text,\n",
    "#         sample_context=sample_context,\n",
    "#         sample_question=sample_question,\n",
    "#         sample_answer=sample_answer,\n",
    "#     )\n",
    "\n",
    "#     # Generate the questions\n",
    "#     response = call_bedrock(bdrk_runtime, \"anthropic.claude-v2:1\", prompt)\n",
    "\n",
    "#     # Parse the output\n",
    "#     questions = parse_output(response)\n",
    "\n",
    "#     # Create a DataFrame for the current set of questions and answers\n",
    "#     new_entries = pd.DataFrame(\n",
    "#         [{\"Context\": text, \"Question\": q, \"Answer\": a} for q, a in questions]\n",
    "#     )\n",
    "\n",
    "#     # Concatenate the new entries with the main DataFrame\n",
    "#     df = pd.concat([df, new_entries], ignore_index=True)\n",
    "\n",
    "# # Now df has all the context, questions, and answers\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7bfa2e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1954"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = texts[869:]\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7069353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = first_869_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "db6b4131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:   1%|▏         | 26/1954 [00:44<42:23,  1.32s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throttling, retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:  11%|█         | 211/1954 [05:30<46:21,  1.60s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throttling, retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:  11%|█         | 213/1954 [05:35<1:03:25,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throttling, retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:  12%|█▏        | 233/1954 [06:14<51:12,  1.79s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throttling, retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:  13%|█▎        | 251/1954 [06:52<41:39,  1.47s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throttling, retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:  14%|█▍        | 279/1954 [07:44<44:35,  1.60s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throttling, retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:  15%|█▍        | 289/1954 [08:11<55:36,  2.00s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throttling, retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:  16%|█▋        | 318/1954 [09:16<57:52,  2.12s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throttling, retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:  17%|█▋        | 328/1954 [09:33<47:27,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throttling, retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:  17%|█▋        | 330/1954 [09:38<55:10,  2.04s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throttling, retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:  20%|█▉        | 382/1954 [11:18<32:09,  1.23s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throttling, retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:  20%|█▉        | 383/1954 [11:20<34:34,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throttling, retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:  21%|██▏       | 419/1954 [12:28<29:53,  1.17s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throttling, retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:  26%|██▋       | 516/1954 [15:12<36:09,  1.51s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throttling, retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 1954/1954 [49:23<00:00,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Context  \\\n",
      "0     Signature Date Printed Name: Jeffrey A. Wilke ...   \n",
      "1     Signature Date Printed Name: Jeffrey A. Wilke ...   \n",
      "2     Item 1. Business\\n\\nThis Annual Report on Form...   \n",
      "3     Item 1. Business\\n\\nThis Annual Report on Form...   \n",
      "4     Item 1. Business\\n\\nThis Annual Report on Form...   \n",
      "...                                                 ...   \n",
      "7926  (1) The Report fully complies with the require...   \n",
      "7927  (1) The Report fully complies with the require...   \n",
      "7928  Exhibit 32.1\\n\\nCertification Pursuant to 18 U...   \n",
      "7929  Exhibit 32.1\\n\\nCertification Pursuant to 18 U...   \n",
      "7930  Exhibit 32.1\\n\\nCertification Pursuant to 18 U...   \n",
      "\n",
      "                                               Question  \\\n",
      "0     What was Richard Dalzell's starting salary whe...   \n",
      "1     How much of a signing bonus did Amazon.com off...   \n",
      "2                What year was Amazon.com incorporated?   \n",
      "3     On what stock exchange is Amazon's common stoc...   \n",
      "4     Through what programs can any business or indi...   \n",
      "...                                                 ...   \n",
      "7926  Who signed the written statement confirming th...   \n",
      "7927  What date was the written statement signed by ...   \n",
      "7928  Who signed the certification pursuant to 18 U....   \n",
      "7929  What date was Andrew R. Jassy's certification ...   \n",
      "7930  Who certified that Amazon.com's 2022 annual re...   \n",
      "\n",
      "                                                 Answer  \n",
      "0     Richard Dalzell's starting salary when he join...  \n",
      "1     Amazon.com offered Richard Dalzell a one-time ...  \n",
      "2     Amazon.com was incorporated in 1994 in the sta...  \n",
      "3     Amazon's common stock is listed on the Nasdaq ...  \n",
      "4     Any business or individual can sell products o...  \n",
      "...                                                 ...  \n",
      "7926  Brian T. Olsavsky, Senior Vice President and C...  \n",
      "7927                                   February 2, 2023  \n",
      "7928  Andrew R. Jassy, President and Chief Executive...  \n",
      "7929  Andrew R. Jassy's certification was signed on ...  \n",
      "7930  Both Andrew R. Jassy, President and CEO, and B...  \n",
      "\n",
      "[7931 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "def parse_output(output):\n",
    "    pattern = r\"<NewPair>\\s*<question>(.*?)</question>\\s*<answer>\\s*(.*?)\\s*</answer>\\s*</NewPair>\"\n",
    "    matches = re.findall(pattern, output, re.DOTALL)\n",
    "    return matches\n",
    "\n",
    "\n",
    "def generate_and_parse(text, prompt_template, max_retries=5):\n",
    "    attempts = 0\n",
    "    while attempts < max_retries:\n",
    "        try:\n",
    "            prompt = prompt_template.format(\n",
    "                context=text,\n",
    "                sample_context=sample_context,\n",
    "                sample_question=sample_question,\n",
    "                sample_answer=sample_answer,\n",
    "            )\n",
    "\n",
    "            # Generate the questions\n",
    "            response = call_bedrock(bdrk_runtime, \"anthropic.claude-v2:1\", prompt)\n",
    "\n",
    "            # Parse the output\n",
    "            questions = parse_output(response)\n",
    "            return text, questions\n",
    "\n",
    "        except Exception as e:  # Catch general exception\n",
    "            if \"ThrottlingException\" in str(\n",
    "                e\n",
    "            ):  # Check if exception message contains 'ThrottlingException'\n",
    "                sleep_time = 60 * (2**attempts)  # Exponential back-off\n",
    "                print(f\"Throttling, retrying in {sleep_time} seconds...\")\n",
    "                sleep(sleep_time)\n",
    "                attempts += 1\n",
    "\n",
    "    return text, []  # Return empty if max retries exceeded\n",
    "\n",
    "\n",
    "# ThreadPoolExecutor for parallel processing\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    # Dictionary to keep track of futures\n",
    "    future_to_text = {\n",
    "        executor.submit(generate_and_parse, text, PromptTemplate): text\n",
    "        for text in texts\n",
    "    }\n",
    "\n",
    "    for future in tqdm(\n",
    "        as_completed(future_to_text), total=len(future_to_text), desc=\"Processing texts\"\n",
    "    ):\n",
    "        text, questions = future.result()\n",
    "\n",
    "        # Create a DataFrame for the current set of questions and answers\n",
    "        new_entries = pd.DataFrame(\n",
    "            [{\"Context\": text, \"Question\": q, \"Answer\": a} for q, a in questions]\n",
    "        )\n",
    "\n",
    "        # Concatenate the new entries with the main DataFrame\n",
    "        df = pd.concat([df, new_entries], ignore_index=True)\n",
    "\n",
    "# Now df has all the context, questions, and answers\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9c46c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Questions_save.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "191b7c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_feather(\"Questions_save.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a66b2418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 3)\n",
      "(2931, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df = df[:5000]\n",
    "monitor_df = df[5000:]\n",
    "\n",
    "print(train_df.shape)\n",
    "print(monitor_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4522a7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save as Q&A Dataset\n",
    "with open(\"qa_dataset_train.jsonl\", \"w\") as file:\n",
    "    for _, row in train_df.iterrows():\n",
    "        qa_entry = {\"question\": row[\"Question\"], \"answer\": row[\"Answer\"]}\n",
    "        file.write(json.dumps(qa_entry) + \"\\n\")\n",
    "\n",
    "# Save as Q&A Dataset\n",
    "with open(\"qa_dataset_monitor.jsonl\", \"w\") as file:\n",
    "    for _, row in monitor_df.iterrows():\n",
    "        qa_entry = {\"question\": row[\"Question\"], \"answer\": row[\"Answer\"]}\n",
    "        file.write(json.dumps(qa_entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "730627d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as Instruction Dataset\n",
    "with open(\"instruction_dataset_train.jsonl\", \"w\") as file:\n",
    "    for _, row in train_df.iterrows():\n",
    "        instruction_entry = {\n",
    "            \"instruction\": row[\"Question\"],\n",
    "            \"context\": row[\"Context\"],\n",
    "            \"response\": row[\"Answer\"],\n",
    "        }\n",
    "        file.write(json.dumps(instruction_entry) + \"\\n\")\n",
    "\n",
    "# Save as Instruction Dataset\n",
    "with open(\"instruction_dataset_monitor.jsonl\", \"w\") as file:\n",
    "    for _, row in monitor_df.iterrows():\n",
    "        instruction_entry = {\n",
    "            \"instruction\": row[\"Question\"],\n",
    "            \"context\": row[\"Context\"],\n",
    "            \"response\": row[\"Answer\"],\n",
    "        }\n",
    "        file.write(json.dumps(instruction_entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c12ac6a-a962-4408-9e75-af360c71cd1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "bedrock39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
