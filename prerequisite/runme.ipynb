{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6496bcb-59a1-4041-bf86-0a19df0a6765",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafd2cea-16a0-4114-a849-6d4cf31081ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from datasets import disable_caching\n",
    "\n",
    "import boto3\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "def print_dashes():\n",
    "    print('-'.join('' for x in range(100)))\n",
    "\n",
    "def translate_to_text(data, column_name=\"context\"):\n",
    "    data_pd = pd.DataFrame(data)\n",
    "    return \" \\n\\n\\n\\n\".join(((data_pd.drop_duplicates(subset=[column_name]))[column_name]))\n",
    "\n",
    "def safe_open_w(path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    return open(path, 'w')\n",
    "\n",
    "def write_to_file(input_str, file_path):\n",
    "    with safe_open_w(file_path) as text_file:\n",
    "        text_file.write(input_str)\n",
    "\n",
    "def create_custom_template(template, file_path):\n",
    "    with safe_open_w(file_path) as text_file:\n",
    "        json.dump(template, text_file)\n",
    "    return template\n",
    "\n",
    "def attach_cors_to_bucket(bucket_name):    \n",
    "    s3 = boto3.client('s3')\n",
    "  \n",
    "    try:\n",
    "        response = s3.put_bucket_cors(Bucket = bucket_name, \n",
    "                                      CORSConfiguration = {\n",
    "                                            'CORSRules' : [\n",
    "                                                {\n",
    "                                                    'ID' : bucket_name + 'cors',\n",
    "                                                    'AllowedHeaders' : [ '*' ],\n",
    "                                                    'AllowedMethods' : [ 'PUT', 'GET', 'POST', 'DELETE', 'HEAD' ],\n",
    "                                                    'AllowedOrigins' : [ '*' ],\n",
    "                                                    'ExposeHeaders' :  [ 'ETag', 'x-amz-delete-marker', 'x-amz-server-side-encryption',\n",
    "                                                                         'x-amz-request-id','x-amz-version-id','x-amz-id-2']\n",
    "                                                }\n",
    "                                            ]\n",
    "                                        })\n",
    "    except ClientError as e:\n",
    "            return None\n",
    "    return response\n",
    "\n",
    "def upload_workshop_dataset(dataset_name,\n",
    "                            output_bucket = sagemaker.Session().default_bucket(),\n",
    "                            local_path = \".\"):\n",
    "    \n",
    "    attach_cors_to_bucket(output_bucket) # requirment by Studio UI\n",
    "    \n",
    "    output_s3_path =  output_bucket + \"/datasets\" \n",
    "    \n",
    "    data_location = f\"s3://{output_s3_path}/\" + dataset_name\n",
    "    \n",
    "    fine_tune_data_ist_location = f\"{data_location}/fine_tuning/instruction_fine_tuning\"\n",
    "    fine_tune_data_daft_location = f\"{data_location}/fine_tuning/domain_adaptation_fine_tuning\"\n",
    "    evaluation_data_location = f\"{data_location}/evaluation\"\n",
    "    \n",
    "    if(os.path.isfile(f\"{local_path}/template.json\")):\n",
    "        print(\"Uploading custom template...\")\n",
    "        S3Uploader.upload(f\"{local_path}/template.json\", fine_tune_data_ist_location)\n",
    "        print(\"Done\")\n",
    "    \n",
    "    if(os.path.isfile(f\"{local_path}/dataset_finetune_ist.jsonl\")):\n",
    "        print(\"Uploading instruction tuning dataset...\")\n",
    "        S3Uploader.upload(f\"{local_path}/dataset_finetune_ist.jsonl\", fine_tune_data_ist_location)\n",
    "        print(f\"Fine-tuning ist data: {fine_tune_data_ist_location}\")\n",
    "    \n",
    "    if(os.path.isfile(f\"{local_path}/dataset_finetune_daft.txt\")):\n",
    "        print(\"Uploading domain adaptation tuning dataset...\")\n",
    "        S3Uploader.upload(f\"{local_path}/dataset_finetune_daft.txt\", fine_tune_data_daft_location)\n",
    "        print(f\"Fine-tuning daft data: {fine_tune_data_daft_location}\")\n",
    "    \n",
    "    if(os.path.isfile(f\"{local_path}/dataset_evaluation.jsonl\")):\n",
    "        print(\"Uploading evaluation dataset...\")\n",
    "        S3Uploader.upload(f\"{local_path}/dataset_evaluation.jsonl\", evaluation_data_location)\n",
    "        print(f\"Evaluation data: {evaluation_data_location}\")\n",
    "\n",
    "\n",
    "dataset_loaded_sciq = False\n",
    "dataset_loaded_squad = False\n",
    "\n",
    "disable_caching()\n",
    "\n",
    "def prepare_dataset_sciq():\n",
    "    dataset_name = 'sciq'\n",
    "    \n",
    "    if(dataset_loaded_sciq == False):\n",
    "        dataset = load_dataset(dataset_name)\n",
    "        dataset_loaded_sciq = True\n",
    "    \n",
    "    dataset_training_df = pd.DataFrame(dataset['train'])\n",
    "    dataset_validation_df = pd.DataFrame(dataset['test'])\n",
    "    \n",
    "    number_of_raws_training = 5000 # dataset_training_df.size\n",
    "    dataset_training_df = dataset_training_df.sample(n=int(number_of_raws_training/len(dataset_training_df.columns)), random_state=42, ignore_index=True)\n",
    "    \n",
    "    #number_of_raws_validation = 2000 # dataset_training_df.size\n",
    "    #dataset_validation_df = dataset_validation_df.sample(n=int(number_of_raws_validation/len(dataset_validation_df.columns)), random_state=42, ignore_index=True)\n",
    "    \n",
    "    print_dashes()\n",
    "    print(\"Load data\")\n",
    "    print_dashes()\n",
    "    print(\"Train dataset size \" + str(dataset_training_df.size) + \" with columns\" + str(dataset_training_df.columns.to_list()) )\n",
    "    print(\"Validation dataset size \" + str(dataset_validation_df.size) + \" with columns\" + str(dataset_validation_df.columns.to_list()) )\n",
    "    print_dashes()\n",
    "    \n",
    "    print(\"\\nCreate DAFT dataset\")\n",
    "    print_dashes()\n",
    "    data_train_daft = translate_to_text(dataset_training_df, column_name='support')\n",
    "    print(\"DAFT dataset example: \" + data_train_daft[0:1000])\n",
    "    print(\"Exoprt DAFT dataset: dataset_finetune_daft.txt\")\n",
    "    write_to_file(data_train_daft, f\"./{dataset_name}/dataset_finetune_daft.txt\")\n",
    "    print_dashes()\n",
    "    \n",
    "    print(\"\\nCreate IST dataset\")\n",
    "    print_dashes()\n",
    "    dataset_train_ist_df = dataset_training_df[['support', 'question', 'correct_answer']].copy()\n",
    "    print(\"IST dataframe example: \" + dataset_train_ist_df.iloc[0])\n",
    "    dataset_fine_tune_ist = Dataset.from_pandas(dataset_train_ist_df)\n",
    "    print(\"IST dataset example: \",dataset_fine_tune_ist[0])\n",
    "    print(\"Exoprt IST dataset: dataset_finetune_ist.jsonl\")\n",
    "    dataset_fine_tune_ist.to_json(f\"./{dataset_name}/dataset_finetune_ist.jsonl\",orient='records', lines=True)\n",
    "    print_dashes()\n",
    "    \n",
    "    print(\"\\nCreate prompt template and store to template.json\")\n",
    "    print_dashes()\n",
    "    template = {\n",
    "            \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{question}\\n\\n### Input:\\n{support}\",\n",
    "            \"completion\": \"{correct_answer}\"\n",
    "        }\n",
    "    print(create_custom_template(template, f\"./{dataset_name}/template.json\"))\n",
    "    print_dashes()\n",
    "    \n",
    "    print(\"\\nCreate evaluation dataset\")\n",
    "    print_dashes()\n",
    "    dataset_validation_df[\"model_input\"] = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n\" + dataset_validation_df[\"question\"] + \"\\n\\n### Input:\\n\" + dataset_validation_df[\"question\"]\n",
    "    dataset_evaluation_df = dataset_validation_df[['model_input','correct_answer']].copy()\n",
    "    dataset_evaluation_df = dataset_evaluation_df.rename(columns={\"correct_answer\": \"target_output\"})\n",
    "    print(\"Evaluation dataframe example: \" + dataset_evaluation_df.iloc[0])\n",
    "    dataset_evaluation = Dataset.from_pandas(dataset_evaluation_df)\n",
    "    print(\"Evaluation dataset example: \", dataset_evaluation[0])\n",
    "    print(\"Exoprt Evaluation dataset: dataset_evaluation.jsonl\")\n",
    "    dataset_evaluation.to_json(f\"./{dataset_name}/dataset_evaluation.jsonl\")\n",
    "    print_dashes()\n",
    "    \n",
    "    upload_workshop_dataset(dataset_name = dataset_name, output_bucket = sagemaker.Session().default_bucket(), local_path = f\"./{dataset_name}\")\n",
    "\n",
    "def prepare_dataset_squad():\n",
    "    dataset_name = 'squad'\n",
    "    \n",
    "    if(dataset_loaded_squad == False):\n",
    "        dataset = load_dataset(dataset_name)\n",
    "        dataset_loaded_squad = True\n",
    "        \n",
    "    dataset_training_df = pd.DataFrame(dataset['train'])\n",
    "    dataset_validation_df = pd.DataFrame(dataset['validation'])\n",
    "    \n",
    "    #number_of_raws_training = 5000 # dataset_training_df.size\n",
    "    #dataset_training_df = dataset_training_df.sample(n=int(number_of_raws_training/len(dataset_training_df.columns)), random_state=42, ignore_index=True)\n",
    "    \n",
    "    #number_of_raws_validation = 2000 # dataset_training_df.size\n",
    "    #dataset_validation_df = dataset_validation_df.sample(n=int(number_of_raws_validation/len(dataset_validation_df.columns)), random_state=42, ignore_index=True)\n",
    "    \n",
    "    print_dashes()\n",
    "    print(\"Load data\")\n",
    "    print_dashes()\n",
    "    print(\"Train dataset size \" + str(dataset_training_df.size) + \" with columns\" + str(dataset_training_df.columns.to_list()) )\n",
    "    print(\"Validation dataset size \" + str(dataset_validation_df.size) + \" with columns\" + str(dataset_validation_df.columns.to_list()) )\n",
    "    print_dashes()\n",
    "    \n",
    "    print(\"\\nCreate DAFT dataset\")\n",
    "    print_dashes()\n",
    "    data_train_daft = translate_to_text(dataset_training_df)\n",
    "    print(\"DAFT dataset example: \" + data_train_daft[0:1000])\n",
    "    print(\"Exoprt DAFT dataset: dataset_finetune_daft.txt\")\n",
    "    write_to_file(data_train_daft, f\"./{dataset_name}/dataset_finetune_daft.txt\")\n",
    "    print_dashes()\n",
    "    \n",
    "    print(\"\\nCreate IST dataset\")\n",
    "    print_dashes()\n",
    "    dataset_train_ist_df = dataset_training_df[['context', 'question', 'answers']].copy()\n",
    "    dataset_train_ist_df['answers'] = dataset_train_ist_df['answers'].apply(lambda x: str(x[\"text\"][0]))\n",
    "    print(\"IST dataframe example: \" + dataset_train_ist_df.iloc[0])\n",
    "    dataset_fine_tune_ist = Dataset.from_pandas(dataset_train_ist_df)\n",
    "    print(\"IST dataset example: \",dataset_fine_tune_ist[0])\n",
    "    print(\"Exoprt IST dataset: dataset_finetune_ist.jsonl\")\n",
    "    dataset_fine_tune_ist.to_json(f\"./{dataset_name}/dataset_finetune_ist.jsonl\")\n",
    "    print_dashes()\n",
    "    \n",
    "    print(\"\\nCreate prompt template and store to template.json\")\n",
    "    print_dashes()\n",
    "    template = {\n",
    "          \"prompt\": \"Given the following context: {context}\\n\\nCould you answer this question: {question} \",\n",
    "          \"completion\": \"{answers}\"\n",
    "        }\n",
    "    print(create_custom_template(template, f\"./{dataset_name}/template.json\"))\n",
    "    print_dashes()\n",
    "    \n",
    "    print(\"\\nCreate evaluation dataset\")\n",
    "    print_dashes()\n",
    "    dataset_validation_df[\"model_input\"] = \"Given the following context:\" + dataset_validation_df[\"context\"] + \"\\n\\nCould you answer this question: \" + dataset_validation_df[\"question\"]\n",
    "    dataset_evaluation_df = dataset_validation_df[['model_input','answers']].copy()\n",
    "    dataset_evaluation_df['answers'] = dataset_evaluation_df['answers'].apply(lambda x: str(' <OR> '.join(data for data in x[\"text\"])))\n",
    "    dataset_evaluation_df = dataset_evaluation_df.rename(columns={\"answers\": \"target_output\"})\n",
    "    print(\"Evaluation dataframe example: \" + dataset_evaluation_df.iloc[0])\n",
    "    dataset_evaluation = Dataset.from_pandas(dataset_evaluation_df)\n",
    "    print(\"Evaluation dataset example: \", dataset_evaluation[0])\n",
    "    print(\"Exoprt Evaluation dataset: dataset_evaluation.jsonl\")\n",
    "    dataset_evaluation.to_json(f\"./{dataset_name}/dataset_evaluation.jsonl\")\n",
    "    print_dashes()\n",
    "    \n",
    "    upload_workshop_dataset(dataset_name = dataset_name, output_bucket = sagemaker.Session().default_bucket(), local_path = f\"./{dataset_name}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    prepare_dataset_sciq()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
